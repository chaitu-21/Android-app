import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt

-----------------------------------
data=pd.read_csv("Google_Stock_Price_Train.csv")
data

-----------------------------------
train = data.iloc[:1260,:]
test = data.iloc[1260:,:]

-----------------------------------
train

-----------------------------------
trainset = train.iloc[:,1:2].values

-----------------------------------
trainset

-----------------------------------
from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler(feature_range = (0,1))
training_scaled = sc.fit_transform(trainset)

-----------------------------------
training_scaled

-----------------------------------
x_train = []
y_train = []

-----------------------------------
for i in range(60,1259):
    x_train.append(training_scaled[i-60:i, 0])
    y_train.append(training_scaled[i,0])
x_train,y_train = np.array(x_train),np.array(y_train)

-----------------------------------
x_train.shape

-----------------------------------
x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))

-----------------------------------
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

-----------------------------------
regressor = Sequential()
regressor.add(LSTM(units = 50,return_sequences = True,input_shape = (x_train.shape[1],1)))

-----------------------------------
regressor.add(Dropout(0.2))

-----------------------------------
regressor.add(LSTM(units = 50,return_sequences = True))
regressor.add(Dropout(0.2))

-----------------------------------
regressor.add(LSTM(units = 50,return_sequences = True))
regressor.add(Dropout(0.2))

-----------------------------------
regressor.add(LSTM(units = 50))
regressor.add(Dropout(0.2))

-----------------------------------
regressor.add(Dense(units = 1))

-----------------------------------
regressor.compile(optimizer = 'adam',loss = 'mean_squared_error')

-----------------------------------
regressor.fit(x_train,y_train,epochs = 100, batch_size = 32)

-----------------------------------
test

-----------------------------------
real_stock_price = test.iloc[:,1:2].values

-----------------------------------
dataset_total = pd.concat((train['Open'],test['Open']),axis = 0)
dataset_total

-----------------------------------
inputs = dataset_total[len(dataset_total) - len(test)-60:].values
inputs

-----------------------------------
inputs = inputs.reshape(-1,1)

-----------------------------------
inputs

-----------------------------------
inputs = sc.transform(inputs)
inputs.shape

-----------------------------------
x_test = []
for i in range(60,185):
    x_test.append(inputs[i-60:i,0])

-----------------------------------
x_test = np.array(x_test)
x_test.shape

-----------------------------------
x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))
x_test.shape

-----------------------------------
predicted_price = regressor.predict(x_test)

-----------------------------------
predicted_price = sc.inverse_transform(predicted_price)
predicted_price

-----------------------------------
plt.plot(real_stock_price,color = 'red', label = 'Real Price')
plt.plot(predicted_price, color = 'blue', label = 'Predicted Price')
plt.title('Google Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Google Stock Price')
plt.legend()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import datetime
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from sklearn.metrics import r2_score
------------------------------------------------------------------
#data=pd.read_csv("/content/Google_Stock_Price_Train.csv",index_col='Date',parse_dates=True)
data=pd.read_csv("Google_Stock_Price_Train.csv")
data
------------------------------------------------------------------
data['Close']=data['Close'].str.replace(',','').astype(float)
data['Volume']=data['Volume'].str.replace(',','').astype(float)
------------------------------------------------------------------
# Getting a summary of missing values for each field/attribute
print(data.isnull().sum())
------------------------------------------------------------------
data
------------------------------------------------------------------
scaler = MinMaxScaler()
data_without_date = data[['Open','High','Low','Close','Volume']]
data_scaled = pd.DataFrame(scaler.fit_transform(data_without_date))
------------------------------------------------------------------
data_scaled
------------------------------------------------------------------
data_scaled.hist()
------------------------------------------------------------------
plt.figure(figsize=(6,6))
sns.heatmap(data_scaled.corr())
------------------------------------------------------------------
data_scaled=data_scaled.drop([1,2,4], axis=1)
data_scaled
------------------------------------------------------------------

plt.figure(figsize=(6,6))
sns.heatmap(data_scaled.corr())
------------------------------------------------------------------
def split_seq_multivariate(sequence, n_past, n_future):
    
    '''
    n_past ==> no of past observations
    n_future ==> no of future observations 
    '''
    x, y = [], [] 
    for window_start in range(len(sequence)):
        past_end = window_start + n_past
        future_end = past_end + n_future
        if future_end > len(sequence):
            break
        # slicing the past and future parts of the window
        past   = sequence[window_start:past_end, :]
        future = sequence[past_end:future_end, -1]
        x.append(past)
        y.append(future)
    
    return np.array(x), np.array(y)
     
------------------------------------------------------------------
# specify the window size
n_steps = 60

data_scaled = data_scaled.to_numpy()
data_scaled.shape
------------------------------------------------------------------
data_scaled
------------------------------------------------------------------
# Next, I am using the split_seq_multivariate function to split the dataset into sequences.

# split into samples
X, y = split_seq_multivariate(data_scaled, n_steps,1)
------------------------------------------------------------------
X
------------------------------------------------------------------
X[1].shape
------------------------------------------------------------------
y
------------------------------------------------------------------
# X is in the shape of [samples, timesteps, features]
print(X.shape)
print(y.shape)

# make y to the shape of [samples]
y=y[:,0]
y.shape
------------------------------------------------------------------
# split into train/test
X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=50)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

------------------------------------------------------------------
# further dividing the training set into train and validation data
X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=30)

print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)
------------------------------------------------------------------
# define RNN model
model = Sequential()
model.add(LSTM(612, input_shape=(n_steps,2)))
model.add(Dense(50, activation='relu'))
model.add(Dense(50, activation='relu'))
model.add(Dense(30, activation='relu'))
model.add(Dense(1))
------------------------------------------------------------------
# The following code line gives a summary of the model we have created, mentioning each layers information.
model.summary()
------------------------------------------------------------------
# compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
------------------------------------------------------------------
# fit the model
history = model.fit(X_train, y_train, epochs=250, batch_size=32, verbose=2, validation_data=(X_val, y_val))  # has used mini batch learning, with batch_s
------------------------------------------------------------------
from matplotlib import pyplot
# plot learning curves
pyplot.title('Learning Curves')
pyplot.xlabel('Epoch')
pyplot.ylabel('Root Mean Squared Error')
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='val')
pyplot.legend()
pyplot.show()
------------------------------------------------------------------
# evaluate the model
mse, mae = model.evaluate(X_test, y_test, verbose=0)
print('MSE: %.3f, RMSE: %.3f, MAE: %.3f' % (mse, np.sqrt(mse), mae)) 
------------------------------------------------------------------
# predicting y_test values
print(X_test.shape)
predicted_values = model.predict(X_test)
print(predicted_values.shape)
# print(predicted_values)     
------------------------------------------------------------------
plt.plot(y_test,c = 'r')
plt.plot(predicted_values,c = 'y')
plt.xlabel('Day')
plt.ylabel('Stock Price Volume')
plt.title('Stock Price Volume Prediction Graph using RNN (LSTM)')
plt.legend(['Actual','Predicted'],loc = 'lower right')
plt.figure(figsize=(10,6))

------------------------------------------------------------------
# evaluating using R squared
R_square = r2_score(y_test, predicted_values) 
 
print(R_square)
